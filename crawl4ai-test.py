import json
import asyncio
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from bs4 import BeautifulSoup
import re
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

from zhipuai import ZhipuAI

# è·å–å‰10ç¯‡æ–‡ç« çš„ç¼–å·ï¼ˆä½¿ç”¨ crawl4ai ç­‰å¾… JS æ¸²æŸ“ï¼‰
async def extract_snumber_from_url(base_url, top_n=10):
    try:
        async with AsyncWebCrawler(verbose=True) as crawler:
            result = await crawler.arun(
                url=base_url,
                wait_for="css:a[href*='/news/']",  # ç­‰å¾…æ–‡ç« é“¾æ¥å‡ºç°
                bypass_cache=True,
            )

            if not result.success:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {result.status_code}")
                return None

            soup = BeautifulSoup(result.html, 'html.parser')
            links = soup.find_all('a', href=True)

            # ä½¿ç”¨é›†åˆå»é‡
            snumbers = set()
            for link in links:
                href = link.get('href')
                if href and '/news/' in href:
                    pattern = r'/zh/news/(\d+)'
                    match = re.search(pattern, href)
                    if match:
                        snumber = int(match.group(1))
                        snumbers.add(snumber)

            if snumbers:
                # é™åºæ’åˆ—ï¼ˆç¼–å·è¶Šå¤§è¶Šæ–°ï¼‰ï¼Œå–å‰Nä¸ª
                sorted_numbers = sorted(snumbers, reverse=True)[:top_n]
                print(f"âœ… æ‰¾åˆ° {len(sorted_numbers)} ä¸ªæ–‡ç« ç¼–å·: {sorted_numbers}")
                return sorted_numbers

            print("âš ï¸ æœªæ‰¾åˆ°æ–‡ç« é“¾æ¥")
            return None

    except Exception as e:
        print(f"âŒ error: {e}")
        return None


async def extract_news_article(news_url):
    """ä½¿ç”¨ CSS é€‰æ‹©å™¨æå–æ–‡ç« å†…å®¹"""

    schema = {
        "name": "AIbase News Article",
        "baseSelector": "article",  # ä¿æŒ article ä½œä¸ºåŸºç¡€èŒƒå›´
        "fields": [
            {
                "name": "title",
                "selector": "h1",  # ç®€å•æœ‰æ•ˆ
                "type": "text",
            },
            {
                "name": "publication_date",
                "selector": "div.text-surface-500 > span:last-child",
                "type": "text",
            },
            {
                "name": "author",
                "selector": "h4 > .text-surface-600",
                "type": "text",
            },
            {
                "name": "content",
                "selector": "div.leading-8.post-content.overflow-hidden",
                "type": "text",
            }
        ],
    }

    extraction_strategy = JsonCssExtractionStrategy(schema)

    async with AsyncWebCrawler(verbose=False) as crawler:
        result = await crawler.arun(
            url=news_url,
            config=CrawlerRunConfig(
                extraction_strategy=extraction_strategy,
                wait_for="css:.post-content",  # ç­‰å¾…å†…å®¹åŠ è½½
                page_timeout=30000,
            ),
            page_timeout=30000,
        )

        if not result.success:
            print("è¯·æ±‚å¤±è´¥")
            return None

        if result.extracted_content is None:
            print(f"    æå–å¤±è´¥")
            print(f"    å·²ä¿å­˜HTMLåˆ°: debug_article.html")
            with open("debug_article.html", "w", encoding="utf-8") as f:
                f.write(result.html if result.html else result.cleaned_html)
            return None

        try:
            extracted_data = json.loads(result.extracted_content)
            # å¦‚æœè¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œå–å‡ºç¬¬ä¸€ä¸ªå…ƒç´ 
            if isinstance(extracted_data, list):
                if len(extracted_data) > 0:
                    return extracted_data[0]
                else:
                    return None
            return extracted_data
        except json.JSONDecodeError as e:
            print(f"JSONè§£æå¤±è´¥: {e}")
            return None

def get_news_summary(data):
    """åŒæ­¥å‡½æ•°ï¼šè°ƒç”¨AIç”Ÿæˆæ–‡ç« æ€»ç»“"""
    API_KEY = "61f915e05dd949e98a94267103c0d9ec.FtcjGX1KzJfIaZZj"
    BASE_URL = "https://open.bigmodel.cn/api/paas/v4"

    client = ZhipuAI(api_key=API_KEY, base_url=BASE_URL)

    system_prompt = """
    ## Goals
    è¯»å–å¹¶è§£æ JSON æ ¼å¼çš„æ–‡ç« ï¼Œæç‚¼å‡ºæ–‡ç« çš„ä¸»æ—¨ï¼Œå½¢æˆæœ€å¤š3å¥ï¼Œæ¨è2å¥çš„ç®€æ´çš„æ¦‚è¿°ã€‚

    ## Constrains:
    æ¦‚è¿°é•¿åº¦ä¸è¶…è¿‡ 80 å­—ï¼Œä¿æŒæ–‡ç« çš„åŸæ„å’Œé‡ç‚¹ã€‚

    ## Skills
    JSON è§£æèƒ½åŠ›ï¼Œæ–‡ç« å†…å®¹ç†è§£å’Œæ€»ç»“èƒ½åŠ›ã€‚

    ## Output Format
    æœ€å¤š3å¥ï¼Œæ¨è2å¥æ¦‚è¿°ï¼Œç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡ 80 å­—ã€‚

    ## Workflow:
    1. è¯»å–å¹¶è§£æ JSON æ ¼å¼çš„æ–‡ç« 
    2. ç†è§£æ–‡ç« å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯
    3. ç”Ÿæˆç®€æ´çš„æ¦‚è¿°ï¼Œæœ€å¤š3å¥ï¼Œæ¨è2å¥ï¼Œä¸è¶…è¿‡ 80 å­—
    """

    try:
        response = client.chat.completions.create(
            model="glm-4.7",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"æ–‡ç« å†…å®¹ï¼š{data}"}
            ],
            top_p=0.7,
            temperature=0.1,
            stream=False
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"    âŒ AIæ€»ç»“å¤±è´¥: {e}")
        return None


async def get_news_summary_async(data):
    """å¼‚æ­¥åŒ…è£…å™¨ï¼šåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥çš„AIè°ƒç”¨"""
    return await asyncio.to_thread(get_news_summary, data)


def save_articles_to_json(articles, filename="articles_data.json"):
    """ä¿å­˜æ–‡ç« åˆ—è¡¨åˆ°JSONæ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
async def main():
    # 1. è·å–æ–‡ç« ç¼–å·
    numbers = await extract_snumber_from_url("https://www.aibase.com/zh/news/", 10)

    if not numbers:
        print("âŒ æœªè·å–åˆ°æ–‡ç« ç¼–å·")
        return

    # 2. æ‹¼æ¥æˆå®Œæ•´URL
    urls = [f"https://www.aibase.com/zh/news/{num}" for num in numbers]
    print(f"\nğŸ“ æ–‡ç« URLåˆ—è¡¨:")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url}")

    # 3. æµæ°´çº¿å¤„ç†ï¼šè·å–æ–‡ç«  -> AIæ€»ç»“ -> ä¿å­˜
    print(f"\nğŸ“¥ å¼€å§‹å¤„ç†æ–‡ç« ...")
    articles = []
    news_summary = ""

    for i, url in enumerate(urls, 1):
        print(f"\n[{i}/{len(urls)}] æ­£åœ¨å¤„ç†: {url}")

        # æ­¥éª¤1: è·å–æ–‡ç« å†…å®¹
        article = await extract_news_article(url)
        if not article or not article.get('title'):
            print(f"    âŒ è·å–å¤±è´¥")
            await asyncio.sleep(1)
            continue

        print(f"    âœ… æ ‡é¢˜: {article.get('title', '')}")

        # æ­¥éª¤2: ç«‹å³è°ƒç”¨AIæ€»ç»“
        print(f"    ğŸ¤– æ­£åœ¨ç”Ÿæˆæ€»ç»“...", end=" ", flush=True)
        summary = await get_news_summary_async(article.get('content', ''))
        article['summary'] = summary

        if summary:
            print(f"âœ…")
            # ç´¯ç§¯æ—©æŠ¥æ ¼å¼
            news_summary += f"{i}.{article.get('title', '')}\n{summary}\n\n"
        else:
            print(f"âŒ")

        # æ­¥éª¤3: æ·»åŠ åˆ°åˆ—è¡¨å¹¶ç«‹å³ä¿å­˜
        articles.append(article)
        save_articles_to_json(articles)
        print(f"    ğŸ’¾ å·²ä¿å­˜åˆ° articles_data.json")

        await asyncio.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

    # 4. è¾“å‡ºæ±‡æ€»
    print(f"\n{'='*60}")
    print(f"âœ… æˆåŠŸå¤„ç† {len(articles)} ç¯‡æ–‡ç« ")
    print(f"{'='*60}")

    for i, article in enumerate(articles, 1):
        print(f"\n{i}. {article.get('title', 'æ— æ ‡é¢˜')}")
        if article.get('publication_date'):
            print(f"   ğŸ“… {article.get('publication_date')}")
        if article.get('summary'):
            print(f"   ğŸ“ æ€»ç»“: {article.get('summary')}")

    # 5. è¾“å‡ºæ—©æŠ¥æ ¼å¼
    print(f"\n{'='*60}")
    print("ğŸ“° æ—©æŠ¥æ±‡æ€»")
    print(f"{'='*60}\n")
    print(news_summary)
    print(f"ğŸ’¾ æœ€ç»ˆæ•°æ®å·²ä¿å­˜åˆ°: articles_data.json")



if __name__ == "__main__":
    asyncio.run(main())